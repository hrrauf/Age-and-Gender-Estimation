{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import argparse\n",
    "from contextlib import contextmanager\n",
    "from keras.utils.data_utils import get_file\n",
    "from models.wide_resnet import WideResNet\n",
    "from models.VGG_16  import VGG_16\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D, Concatenate, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from script.random_eraser import get_random_eraser\n",
    "import itertools\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(filepath):\n",
    "    test_img = image.load_img(filepath, target_size=(224, 224))\n",
    "    test_img = image.img_to_array(test_img)\n",
    "   \n",
    "    test_img = np.expand_dims(test_img, axis = 0)\n",
    "\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Initialize Models  e.g 'vgg' or 'wide resnet'\n",
    "random_erase = False\n",
    "test_utfk=False\n",
    "test_wiki=True\n",
    "model_name='wide_resnet'\n",
    "if model_name=='vgg':\n",
    "    print(\"Model selected: VGG\")\n",
    "    age_model=VGG_16(101)(pretrained=True)\n",
    "    age_model.load_weights('./trained_models/age_model_weights.h5')\n",
    "    gender_model=VGG_16(2)(pretrained=True)\n",
    "    gender_model.load_weights('./trained_models/gender_model_weights.h5')\n",
    "    img_size=224\n",
    "    print('models initialized')\n",
    "\n",
    "elif model_name=='wide_resnet':\n",
    "    print(\"Model selected: wide resnet\")\n",
    "    model= WideResNet(64, depth=16, k=8)()\n",
    "    model.load_weights('./trained_models/weights.28-3.73.hdf5')\n",
    "    img_size=64\n",
    "    print('models initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_size=9000\n",
    "\n",
    "if test_utfk:\n",
    "    print(\"Executing test on utfk data\")\n",
    "    path = \"./dataset/crop_part1/\"\n",
    "    files = os.listdir(path)\n",
    "    size = len(files)\n",
    "#     test_size=1000\n",
    "    import cv2\n",
    "    images = []\n",
    "    ages = []\n",
    "    genders = []\n",
    "    for i, file in tqdm(enumerate(files)):\n",
    "        if i == test_size:\n",
    "            break\n",
    "        \n",
    "        split_var = file.split('_')\n",
    "        if(int(split_var[1])==3):\n",
    "            print(\"Invalid value 3. Skipping this iteration\")\n",
    "            continue\n",
    "        image = cv2.imread(path+file)\n",
    "  \n",
    "        image = cv2.resize(image,dsize=(img_size,img_size))\n",
    "        image = image.reshape((image.shape[0],image.shape[1],3))\n",
    "        images.append(image)\n",
    "        \n",
    "        ages.append(split_var[0])\n",
    "        genders.append(int(split_var[1]) )\n",
    "            \n",
    "    import numpy as np\n",
    "    test_x=np.array(images)\n",
    "    test_y=ages\n",
    "    genders = keras.utils.to_categorical(genders, 2)\n",
    "elif test_wiki:\n",
    "    print(\"Executing test on wiki data\")\n",
    "    features=torch.load('./dataset/features.pt').numpy()\n",
    "    target=torch.load('./dataset/age.pt').numpy()\n",
    "    gender=torch.load('./dataset/gender.pt').numpy()\n",
    "    test_idx=random.choices(np.arange(features.shape[0]),k=test_size)\n",
    "    test_x=features[test_idx,...]\n",
    "    test_y=target[test_idx,...]\n",
    "    genders=gender[test_idx,...]\n",
    "    genders = keras.utils.to_categorical(genders, 2)\n",
    "    del features\n",
    "    del target \n",
    "    del gender\n",
    "    if img_size==64:\n",
    "        test_x=np.concatenate([cv2.resize(img, (64,64))[None,:,:,:] for img in tqdm(test_x)], axis=0)\n",
    "        \n",
    "if random_erase == True:\n",
    "    print(\"Adding noise to test data\")\n",
    "    datagen = ImageDataGenerator(\n",
    "                    horizontal_flip=0.3,\n",
    "                    rotation_range=60,\n",
    "                    featurewise_center=False,\n",
    "                    samplewise_center=False,\n",
    "                    featurewise_std_normalization=False,\n",
    "                    samplewise_std_normalization=False,\n",
    "                    zca_whitening=False,\n",
    "                    zca_epsilon=1e-06,\n",
    "                    width_shift_range=0.3,\n",
    "                    height_shift_range=0.3,\n",
    "                    brightness_range=None,\n",
    "                    shear_range=0.1,\n",
    "                    zoom_range=0.2,\n",
    "                    channel_shift_range=0.0,\n",
    "                    fill_mode=\"nearest\",\n",
    "                    cval=0.0,\n",
    "                    vertical_flip=True,\n",
    "                    rescale=None,\n",
    "                    preprocessing_function=None,\n",
    "                    data_format=None,\n",
    "                    validation_split=0.0,\n",
    "                    dtype=None\n",
    "                )\n",
    "    datagen.fit(test_x)\n",
    "    \n",
    "    for img in tqdm(datagen.flow(test_x, batch_size=test_size)):\n",
    "        test_x = img\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name=='vgg':\n",
    "    print(\"Normalizing test data\")\n",
    "    test_x=test_x/255\n",
    "    predictions = age_model.predict(test_x)\n",
    "    gender_predictions = gender_model.predict(test_x)\n",
    "    \n",
    "elif  model_name=='wide_resnet':\n",
    "    predictions=model.predict(test_x)\n",
    "    gender_predictions=predictions[0]\n",
    "    predictions=predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indexes = np.array([i for i in range(0, 101)])\n",
    "apparent_predictions = np.sum(predictions * output_indexes, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = 0\n",
    "actual_mean = 0\n",
    "\n",
    "for i in range(0 ,apparent_predictions.shape[0]):\n",
    "    prediction = int(apparent_predictions[i])\n",
    "    actual = round(int(test_y[i]))\n",
    "    \n",
    "    abs_error = abs(prediction - actual)\n",
    "    actual_mean = actual_mean + actual\n",
    "    \n",
    "    mae = mae + abs_error\n",
    "    \n",
    "mae = mae / apparent_predictions.shape[0]\n",
    "\n",
    "print(\"mae: \",mae)\n",
    "print(\"instances: \",apparent_predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_list = []; actual_list = []\n",
    "\n",
    "for i in gender_predictions:\n",
    "    if(test_utfk==True):\n",
    "        pred_list.append(np.argmin(i))\n",
    "    else:\n",
    "        pred_list.append(np.argmax(i))\n",
    "\n",
    "for i in genders: \n",
    "    actual_list.append(np.argmax(i))\n",
    "\n",
    "cmt=confusion_matrix(actual_list, pred_list)\n",
    "print(classification_report(actual_list, pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='YlGnBu')\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"red\" if cm[i, j] > thresh else \"black\", fontsize = 11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize = 15)\n",
    "    plt.xlabel('Predicted label', fontsize = 15)\n",
    "    \n",
    "plt.figure(figsize=(4,4))\n",
    "plot_confusion_matrix(cmt, classes=np.arange(2),normalize=True)\n",
    "plt.title('Confusion Plot for '+ model_name, fontsize='15')\n",
    "plt.savefig('confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pos = np.arange(101)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "margin=0.4\n",
    "\n",
    "video=False\n",
    "#enter directory to be tested\n",
    "image_dir='./data/test_img/'\n",
    "\n",
    "@contextmanager\n",
    "\n",
    "def video_capture(*args, **kwargs):\n",
    "    cap = cv2.VideoCapture(*args, **kwargs)\n",
    "    try:\n",
    "        yield cap\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "def yield_images():\n",
    "\n",
    "    with video_capture(0) as cap:\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "            if not ret:\n",
    "\n",
    "                raise RuntimeError(\"Failed to capture image\")\n",
    "            yield img\n",
    "\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=1.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 0, 255), 3)\n",
    "\n",
    "def yield_images_from_dir(image_dir):\n",
    "    image_dir = Path(image_dir)\n",
    "    for image_path in image_dir.glob(\"*.*\"):\n",
    "        print(image_path)\n",
    "        img = cv2.imread(str(image_path), 1)\n",
    "        yield img\n",
    "\n",
    "\n",
    "image_generator =  yield_images() if video  else   yield_images_from_dir(image_dir)\n",
    "k=0\n",
    "\n",
    "for k, image in enumerate(image_generator) :\n",
    "     \n",
    "        img=cv2.resize(image, (img_size,img_size))\n",
    "        input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img_h, img_w, _ = np.shape(input_img)\n",
    "        ih, iw, _ = np.shape(image)\n",
    "\n",
    "        detected = detector(image, 1)\n",
    "        faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "        for i, d in enumerate(detected):\n",
    "                    d=detected[i]\n",
    "                    x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "                    xw1 = max(int(x1 - margin * w), 0)\n",
    "                    yw1 = max(int(y1 - margin * h), 0)\n",
    "                    xw2 = min(int(x2 + margin * w), iw - 1)\n",
    "                    yw2 = min(int(y2 + margin * h), ih - 1)\n",
    "                    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 4)\n",
    "         \n",
    "                    faces[i, :, :, :] = cv2.resize(image[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "                    if model_name=='vgg':\n",
    "                    \n",
    "                        results_age = age_model.predict(np.expand_dims(faces[i,:,:,:],axis=0))\n",
    "                        predicted_genders=gender_model.predict(np.expand_dims(faces[i,:,:,:],axis=0))\n",
    "                    elif model_name=='wide_resnet':\n",
    "                        results=model.predict(np.expand_dims(faces[i,:,:,:],axis=0))\n",
    "                        results_age=results[1]\n",
    "                        predicted_genders=results[0]\n",
    "                        \n",
    "                    plt.show()\n",
    "                    ages = np.arange(0, 101).reshape(101, 1)\n",
    "                    predicted_ages = results_age[0].dot(ages).flatten()\n",
    "                    print(predicted_ages)\n",
    "                    label = \"{}, {}\".format(int(predicted_ages),\"M\" if np.argmax(predicted_genders)==1 else \"F\")\n",
    "                    draw_label(image, (d.left(), d.top()), label)\n",
    "        \n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
